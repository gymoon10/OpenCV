{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7a9dd3",
   "metadata": {},
   "source": [
    "## 그랩컷 (GrabCut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585803fe",
   "metadata": {},
   "source": [
    "그래프 컷(graph cut) 기반 **영역 분할** 알고리즘\n",
    "\n",
    "영상의 픽셀을 그래프의 정점(node)으로 간주하고, 픽셀들을 두 개의 그룹(객체/배경)으로 나누는 최적의 컷을 찾는 방식 (Max Flow Minimum Cut)\n",
    "\n",
    "사용자가 지정한 전경/배경 정보를 활용하여 영상 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1958a",
   "metadata": {},
   "source": [
    "**그래프 컷**\n",
    "\n",
    "그래프가 존재할 때 그래프의 정점들을 두 개의 서브 집합으로 나누는 segmenting 작업\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143730767-fa4c0cab-be35-486b-a543-4cbbfbede763.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50f5384",
   "metadata": {},
   "source": [
    "**그랩컷**\n",
    "\n",
    "그래프 컷 기반 영역 분할 알고리즘\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143730791-0520cb08-0065-4f92-914a-202c153c7f57.png)\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143730802-5cdd1d55-2a1e-4144-871f-de387bfa6c21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5a53a",
   "metadata": {},
   "source": [
    "**그랩컷 함수**\n",
    "\n",
    "cv2.grabCut(img, mask, rect, bgdModel, fgdModel, iterCount, mode=None) -> mask, bgdModel, fgdModel\n",
    "\n",
    "`img` : 입력 영상\n",
    "\n",
    "`mask` : 입출력 마스크 (GC_BGD(0), GC_FGD(1), GC_PR_BGD(2), GC_PR_FGD(3))\n",
    "  - 0~3의 값 만을 갖는 행렬\n",
    "  - BGD/FGD : 배경/전경, PR : 아마도~일 것이다\n",
    "\n",
    "`rect` : ROI 영역\n",
    "\n",
    "`bgdModel` : 임시 배경 모델 행렬. 같은 영상 처리 시에는 변경 금지\n",
    "\n",
    "`fgdModel` : 임시 전경 모델 행렬. 같은 영상 처리 시에는 변경 금지\n",
    "\n",
    "  - grabCut 함수는 segmenting 한 결과를 가지고 또 다시 segmenting하여 최적의 결과를 출력\n",
    "  - bgdModel, fgdModel을 지정하고 이를 이용해 recursive하게 segmenting 구현\n",
    "\n",
    "`iterCount` : 결과 생성을 위한 Segmenting 반복 횟수\n",
    "\n",
    "`mode` : cv2.GC_로 시작하는 모드 상수. cv2.GC_INIT_WITH_RECT 또는 cv2.GC_INIT_WITH_MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28478b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rc (145, 155, 360, 194)\n",
      "mask2 shape (426, 640)\n",
      "src shape (426, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "src = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/nemo.jpg')\n",
    "\n",
    "if src is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 사각형 지정을 통해 객체를 지정\n",
    "rc = cv2.selectROI(src)  # 영상에서 객체에 해당하는 부분을 사용자로부터 사각형 모양으로 지정 받아, 해당 사각형에 대한 정보를 반환\n",
    "print('rc', rc)\n",
    "mask = np.zeros(src.shape[:2], np.uint8)  # 입력 영상과 동일한 크기로 마스크 영상 초기화\n",
    "\n",
    "cv2.grabCut(src, mask, rc, None, None, 5, cv2.GC_INIT_WITH_RECT)  # mask에 배경 값, 배경일 거 같은 값, 객체 값 등등 자동으로 지정\n",
    "\n",
    "# 0: cv2.GC_BGD, 2: cv2.GC_PR_BGD\n",
    "mask2 = np.where((mask == 0) | (mask == 2), 0, 1).astype('uint8')   # mask에 배경인 부분(0), 배경일 것 같은 부분(2)을 검정색(0)으로 지정, 나머지는 하얀색(1)으로 지정\n",
    "print('mask2 shape', mask2.shape)\n",
    "print('src shape', src.shape)\n",
    "\n",
    "# mask 연산\n",
    "dst = src * mask2[:, :, np.newaxis]  # newaxis를 적용하여 동일한 차원으로 만든 후 곱셈 (객체를 제외한 부분은 모두 0, 검정색)\n",
    "\n",
    "# 초기 분할 결과 출력\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa45a3",
   "metadata": {},
   "source": [
    "**mask 영상**\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143731147-8db7af8b-c571-496d-bd22-a30dfc3408d6.png)\n",
    "\n",
    "\n",
    "- 배경 부분은 검정색, 배경일 것 같은 부분은 회색, 객체 부분은 흰색에 가까운 회색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1235734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import io\n",
    "\n",
    "src = io.imread('messi5.jpg')\n",
    "\n",
    "if src is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "    \n",
    "# 사각형 지정을 통한 초기 분할\n",
    "mask = np.zeros(src.shape[:2], np.uint8)  # 마스크\n",
    "bgdModel = np.zeros((1, 65), np.float64)  # 배경 모델 ((1, 65) 형태로 미리 생성)\n",
    "fgdModel = np.zeros((1, 65), np.float64)  # 전경 모델\n",
    "\n",
    "rc = cv2.selectROI(src)\n",
    "\n",
    "cv2.grabCut(src, mask, rc, bgdModel, fgdModel, 1, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# 0: cv2.GC_BGD, 2: cv2.GC_PR_BGD\n",
    "mask2 = np.where((mask == 0) | (mask == 2), 0, 1).astype('uint8')\n",
    "dst = src * mask2[:, :, np.newaxis]\n",
    "\n",
    "# 초기 분할 결과 출력\n",
    "cv2.imshow('dst', dst)\n",
    "\n",
    "# 마우스 이벤트 처리 함수 등록\n",
    "def on_mouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:  # 왼쪽 버튼은 전경\n",
    "        cv2.circle(dst, (x, y), 3, (255, 0, 0), -1)  # 파랑색 색칠\n",
    "        cv2.circle(mask, (x, y), 3, cv2.GC_FGD, -1)  # 마스크에 전경 강제 \n",
    "        cv2.imshow('dst', dst)\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:  # 오른쪽 버튼은 배경\n",
    "        cv2.circle(dst, (x, y), 3, (0, 0, 255), -1)  # 빨강색 원\n",
    "        cv2.circle(mask, (x, y), 3, cv2.GC_BGD, -1)  # 마스크에 배경 강제 \n",
    "        cv2.imshow('dst', dst)\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:  # 마우스 움직임\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:  # 왼쪽 누르고 움직이면 전경\n",
    "            cv2.circle(dst, (x, y), 3, (255, 0, 0), -1)\n",
    "            cv2.circle(mask, (x, y), 3, cv2.GC_FGD, -1)\n",
    "            cv2.imshow('dst', dst)\n",
    "        elif flags & cv2.EVENT_FLAG_RBUTTON:  # 오른쪽 누르고 움직이면 배경\n",
    "            cv2.circle(dst, (x, y), 3, (0, 0, 255), -1)\n",
    "            cv2.circle(mask, (x, y), 3, cv2.GC_BGD, -1)\n",
    "            cv2.imshow('dst', dst)\n",
    "            \n",
    "cv2.setMouseCallback('dst', on_mouse)\n",
    "\n",
    "while True:\n",
    "    key = cv2.waitKey()\n",
    "    if key == 13:  # ENTER\n",
    "        # 사용자가 지정한 전경/배경 정보를 활용하여 영상 분할\n",
    "        cv2.grabCut(src, mask, rc, bgdModel, fgdModel, 1, cv2.GC_INIT_WITH_MASK)\n",
    "        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "        dst = src * mask2[:, :, np.newaxis]\n",
    "        cv2.imshow('dst', dst)\n",
    "    elif key == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9315f2",
   "metadata": {},
   "source": [
    "## 모멘트 기반 객체 검출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483f33a",
   "metadata": {},
   "source": [
    "**모멘트**\n",
    "\n",
    "**영상의 모양, 형태 정보를 표현**하는 일련의 실수값들의 집합으로 해당 영상의 특징 벡터를 추출하는 방법\n",
    "\n",
    "특정 함수 집합과의 상관 관계 형태로 계산\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Hu의 7개 불변 모멘트**\n",
    "\n",
    "정규화된 중심 모멘트 값을 조합하여 만든 7개의 모멘트 값\n",
    "\n",
    "**영상의 크기, 회전, 이동, 대칭 변환에 불변**하기 때문에 두 영상에서 원하는 객체를 비교, 인식 할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1c278",
   "metadata": {},
   "source": [
    "**모양 비교 함수**\n",
    "\n",
    "cv2.matchShapes(contour1, contour2, method, parameter) -> retval\n",
    "\n",
    "`contour1` : 첫 번째 외곽선 또는 그레이스케일 영상\n",
    "\n",
    "`contour2` : 두 번째 외곽선 또는 그레이스케일 영상\n",
    "\n",
    "`method` : 비교 방법 지정 (CONTOURS_MATCH_I1 / MATCH_I2 / MATCH_I3)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143732551-58b24fd5-fc96-4f6b-80a2-bfac44259b14.png)\n",
    "\n",
    "`parameter` : 0\n",
    "\n",
    "`retval` : 두 외곽선 또는 그레이 스케일 영상 간의 벡터 거리\n",
    "\n",
    "    - 두 객체가 비슷하면 특징을 나타내는 모멘트 벡터의 값이 유사하여 값의 차이가 작음, 다를 수록 벡터 값이 다르기 때문에 차이가 크게 나타남\n",
    "\n",
    "HU의 불변 모멘트를 사용하여 두 외곽선 또는 영상의 모양을 비교\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "789f23f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# symbols에서 spade 찾기\n",
    "obj = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/spades.png', cv2.IMREAD_GRAYSCALE)  # 스페이드\n",
    "src = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/symbols.png', cv2.IMREAD_GRAYSCALE)  # 스페이드를 비롯한 다양한 도형들\n",
    "\n",
    "if src is None or obj is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 객체 영상 외곽선 검출\n",
    "_, obj_bin = cv2.threshold(obj, 128, 255, cv2.THRESH_BINARY_INV)  # 영상의 색을 반전 (검은색 도형들), 이진화\n",
    "obj_contours, _ = cv2.findContours(obj_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # 외곽선 검출 (바깥 윤곽선, 모든 외곽선 좌표 구하기)\n",
    "obj_pts = obj_contours[0]\n",
    "\n",
    "# 입력 영상 분석\n",
    "_, src_bin = cv2.threshold(src, 128, 255, cv2.THRESH_BINARY_INV)\n",
    "contours, _ = cv2.findContours(src_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  # 모든 객체의 외곽선 정보를 저장\n",
    "\n",
    "# 결과 영상\n",
    "dst = cv2.cvtColor(src, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# 입력 영상의 모든 객체의 외곽선 정보에 대해서\n",
    "# 입력 영상의 모든 객체 영역에 대해 contours에 있는 각각의 점들의 집합을 검출\n",
    "for pts in contours:\n",
    "    if cv2.contourArea(pts) < 1000:  # 너무 작은 노이즈 객체는 배제\n",
    "        continue\n",
    "\n",
    "    rc = cv2.boundingRect(pts)  # 외곽선의 좌표를 입력하면 사각형 좌표 반환\n",
    "    cv2.rectangle(dst, rc, (255, 0, 0), 1)\n",
    "\n",
    "    # 모양 비교\n",
    "    dist = cv2.matchShapes(obj_pts, pts, cv2.CONTOURS_MATCH_I3, 0)\n",
    "\n",
    "    cv2.putText(dst, str(round(dist, 4)), (rc[0], rc[1] - 3),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    if dist < 0.1:\n",
    "        cv2.rectangle(dst, rc, (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('obj', obj)\n",
    "cv2.imshow('dst', dst)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabf601",
   "metadata": {},
   "source": [
    "## 템플릿 매칭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ddb909",
   "metadata": {},
   "source": [
    "**템플릿** : 찾을 대상이 되는 작은 영상. patch\n",
    "\n",
    "입력 영상에서 작은 크기의 부분(템플릿) 영상과 일치하는 부분을 찾는 기법\n",
    "\n",
    "작은 크기의 템플릿 영상을 입력 영상 전체 영역에 대해 이동하면서 가장 비슷한 위치를 수치적으로 찾아내는 방식\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143732907-2eaa9f4e-e205-4a40-8442-c118cc71bcb5.png)\n",
    "\n",
    "- 템플릿 영상을 입력 영상의 전체 영역에 대해 이동하면서 템플릿 영상과 입력 영상 간의 유사도/비유사도를 계산\n",
    "\n",
    "- 유사도는 템플릿 영상과 비슷한 부분 영상 위치에서 값이 크게 나타남\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e78ea91",
   "metadata": {},
   "source": [
    "cv2.matchTemplate(image, temp1, method, result=None, mask=None) -> result\n",
    "\n",
    "`image` : 입력 영상 (W x H)\n",
    "\n",
    "`templ` : 템플릿 영상. image보다 같거나 작은 크기, 같은 타입 (w x h)\n",
    "\n",
    "`method` : 비교 방법. cv2.TM_으로 시작하는 플래그 지정\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143733067-4a31caa9-021a-42ae-9790-c9f93a15c480.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143733218-058670a2-21e9-476c-ac7e-f9c5514ce322.png)\n",
    "\n",
    "\n",
    "`result` : 비교 결과 행렬. numpy.ndarray. dtype=numpy.float32 (W - w + 1 x H - h + 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30355c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxv: 0.9796523451805115\n",
      "maxloc: (568, 320)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 입력 영상 & 템플릿 영상\n",
    "src = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/circuit.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "templ = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/crystal.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if src is None or templ is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 입력 영상 밝기 50증가, 가우시안 잡음(sigma=10) 추가\n",
    "noise = np.zeros(src.shape, np.int32)\n",
    "cv2.randn(noise, 50, 10)\n",
    "src = cv2.add(src, noise, dtype=cv2.CV_8UC3)\n",
    "\n",
    "# 템플릿 매칭 & 결과 분석\n",
    "res = cv2.matchTemplate(src, templ, cv2.TM_CCOEFF_NORMED)\n",
    "res_norm = cv2.normalize(res, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "# 최소/최대 위치 함수 (최소 포인터, 최대 포인터, 최소 지점, 최대 지점)\n",
    "_, maxv, _, maxloc = cv2.minMaxLoc(res)\n",
    "print('maxv:', maxv)\n",
    "print('maxloc:', maxloc)  # 좌측 상단 모서리 좌표\n",
    "\n",
    "# 매칭 결과를 빨간색 사각형으로 표시\n",
    "th, tw = templ.shape[:2]\n",
    "dst = cv2.cvtColor(src, cv2.COLOR_GRAY2BGR)\n",
    "cv2.rectangle(dst, maxloc, (maxloc[0] + tw, maxloc[1] + th), (0, 0, 255), 2)\n",
    "\n",
    "# 결과 영상 화면 출력\n",
    "cv2.imshow('res_norm', res_norm)  # 유사도의 그레이 스케일 영상\n",
    "cv2.imshow('dst', dst)  # 매칭 결과 영상\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae608941",
   "metadata": {},
   "source": [
    "cv2.minMaxLoc(res)\n",
    "\n",
    "\n",
    "- cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED는 최소 지점(minLoc)이 검출된 위치\n",
    "\n",
    "\n",
    "- cv2.TM_CCORR, cv2.TM_CCORR_NORMED, cv2.TM_CCOEFF, cv2.TM_CCOEFF_NORMED는 최대 지점(maxLoc)이 검출된 위치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91f670",
   "metadata": {},
   "source": [
    "### 템플릿 매칭 - 인쇄체 숫자 인식\n",
    "\n",
    "\n",
    "템플릿 영상 : Consolas 폰트로 쓰여진 0~9 숫자 영상 (100 x 150)\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143733415-30e0b220-f3a2-42d1-a36b-993e960ccf36.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a051741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 숫자 템플릿 영상 digit0.bmp ~ digit9.bmp 불러오기\n",
    "def load_digits():\n",
    "    img_digits = []\n",
    "\n",
    "    for i in range(10):\n",
    "        filename = 'C:/Users/ky_moon/Desktop/vision/ch08/digits/digit{}.bmp'.format(i)\n",
    "        img_digits.append(cv2.imread(filename, cv2.IMREAD_GRAYSCALE))\n",
    "\n",
    "        if img_digits[i] is None:\n",
    "            return None\n",
    "\n",
    "    return img_digits\n",
    "\n",
    "\n",
    "def find_digit(img, img_digits):\n",
    "    max_idx = -1\n",
    "    max_ccoeff = -1\n",
    "\n",
    "    # 최대 NCC 찾기\n",
    "    for i in range(10):\n",
    "        img = cv2.resize(img, (100, 150))\n",
    "        res = cv2.matchTemplate(img, img_digits[i], cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "        if res[0, 0] > max_ccoeff:\n",
    "            max_idx = i\n",
    "            max_ccoeff = res[0, 0]\n",
    "\n",
    "    return max_idx\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 입력 영상 불러오기\n",
    "    src = cv2.imread('digits_print.bmp')\n",
    "\n",
    "    if src is None:\n",
    "        print('Image load failed!')\n",
    "        return\n",
    "\n",
    "    # 100x150 숫자 템플릿 영상 불러오기\n",
    "    img_digits = load_digits()  # list of ndarray\n",
    "\n",
    "    if img_digits is None:\n",
    "        print('Digit image load failed!')\n",
    "        return\n",
    "\n",
    "    # 입력 영상 이진화 & 레이블링\n",
    "    src_gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "    _, src_bin = cv2.threshold(src_gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    cnt, _, stats, _ = cv2.connectedComponentsWithStats(src_bin)  # 객체 개수, 바운딩 박스 정보\n",
    "\n",
    "    # 숫자 인식 결과 영상 생성\n",
    "    dst = src.copy()\n",
    "    for i in range(1, cnt):\n",
    "        (x, y, w, h, s) = stats[i]\n",
    "\n",
    "        if s < 1000:\n",
    "            continue\n",
    "\n",
    "        # 가장 유사한 숫자 이미지를 선택\n",
    "        digit = find_digit(src_gray[y:y+h, x:x+w], img_digits)\n",
    "        cv2.rectangle(dst, (x, y, w, h), (0, 255, 255))\n",
    "        cv2.putText(dst, str(digit), (x, y - 4), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1, (0, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('dst', dst)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9015d",
   "metadata": {},
   "source": [
    "## 캐스케이드 분류기 : 얼굴 검출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd414bb8",
   "metadata": {},
   "source": [
    "영상을 24 x 24 크기로 정규화한 후, 유사-하르 필터 (Haar-like) 집합으로부터 특징 정보를 추출하여 얼굴 여부를 판별\n",
    "\n",
    "AdaBoost에 기반한 강한 분류 성능 & 캐스케이드 방식을 통한 빠른 동작\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "\n",
    "**유사 하르 필터**\n",
    "\n",
    "흑백 사각형이 서로 붙어 있는 형태로 구성된 필터\n",
    "\n",
    "흰색 영역 픽셀 값은 모두 더하고, 검은색 영역 픽셀 값은 모두 빼서 하나의 특징 값 계산\n",
    "\n",
    "유사 하르 필터로 구한 특징 값을 통해 얼굴을 판별\n",
    "\n",
    "  - 얼굴 형태는 전형적으로 밝은 영역(이마, 미간, 볼 등), 어두운 영역(눈썹, 입술 등)으로 정해져 있음\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143734100-730dc9b1-e4a0-487d-bace-910c2d6f0dab.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143734183-894340db-9471-40ea-a36d-953c9f869d40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf92a8",
   "metadata": {},
   "source": [
    "**캐스케이드 분류기**\n",
    "\n",
    "\n",
    "일반적인 영상에서 대부분의 영역이 non-face 영역\n",
    "\n",
    "non face영역을 빠르게 skip하고 넘어갈 수 있도록 다단계 검사 수행\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143734491-8d4754b7-d6a0-4f67-bc95-1875b0e69e16.png)\n",
    "\n",
    "\n",
    " - 특정 단계에서 얼굴이 아니라고 판단되면 더 이상 검사를 수행하지 않음, 통과하면 다음 단계로 pass \n",
    " \n",
    " \n",
    "참고 : https://www.youtube.com/watch?v=hPCTwxF0qf4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b3511",
   "metadata": {},
   "source": [
    "**객체 생성 및 학습 데이터 불러오기 함수 - cv2.CascadeClassifier()**\n",
    "\n",
    "미리 학습된 정보를 불러와서 내가 찾고자 하는 객체를 검출\n",
    "\n",
    "filename으로 저장된, 객체의 특징을 담고 있는 파일을 불러올 수 있음\n",
    "\n",
    "cv2.CascadeClassifier( ) -> CascadeClassifier object    \n",
    "cv2.CascadeClassifier(filename) -> CascadeClassifier object \n",
    "\n",
    "cv2.CascadeClassifier.load(filename) -> retval (filename을 지정했으면 load 할 필요가 x)\n",
    "\n",
    "`filename` : XML 파일 이름\n",
    "\n",
    "`retval` : 성공하면 True, 실패하면 False\n",
    "\n",
    "\n",
    "미리 학습된 XML 파일 다운로드 : https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143734651-59209441-f874-4906-9c4b-1fe0678018c1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb5c72",
   "metadata": {},
   "source": [
    "**CascadeClassifier 멀티 스케일 객체 검출 함수**\n",
    "\n",
    "cv2.CascadeClassifier.detectMultiScale(image, scaleFactor=None, minNeighbors=None, flags=None, minSize=None, maxSize=None) -> result\n",
    "\n",
    "`image` : 입력 영상 (cv2.CV_8U)\n",
    "\n",
    "`scaleFactor` : 영상 축소 비율. 기본값은 1.1.\n",
    "\n",
    "`minNeighbors` : 얼마나 많은 이웃 사각형이 검출되어야 최종 검출 영역으로 설정할지를 지정. 기본값은 3.\n",
    "\n",
    "`flags` : (현재) 사용되지 않음\n",
    "\n",
    "`minSize` : 최소 객체 크기. (w, h) 튜플.\n",
    "\n",
    "`maxSize` : 최대 객체 크기. (w, h) 튜플.\n",
    "\n",
    "`result` : 검출된 객체의 사각형 정보(x, y, w, h)를 담은 numpy.ndarray. shape=(N, 4). dtype=numpy.int32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1a43b3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "src = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/lenna.bmp')\n",
    "\n",
    "if src is None:\n",
    "    print('Image load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "classifier = cv2.CascadeClassifier('C:/Users/ky_moon/Desktop/vision/ch08/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "if classifier.empty():\n",
    "    print('XML load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "faces = classifier.detectMultiScale(src)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(src, (x, y, w, h), (255, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "554a3a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[217, 200, 179, 179]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c627d",
   "metadata": {},
   "source": [
    "## HOG 알고리즘 & 보행자 검출 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72039122",
   "metadata": {},
   "source": [
    "**Histogram of Oriented Gradients**\n",
    "\n",
    "영상의 지역적 그래디언트 방향 정보를 특징 벡터로 사용\n",
    "\n",
    "사람이 서 있는 영상에서 그래디언트를 구하고, 그래디언트의 크기와 방향 성분을 이용하여 사람이 서 있는 형태에 대한 특징 벡터를 정의\n",
    "\n",
    "  - SVM을 이용하여 입력 영상에서 보행자 위치를 검출\n",
    "\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/44194558/143734934-d11653a2-ad27-463f-bc6b-81f9205c71c2.png)\n",
    "\n",
    "\n",
    "\n",
    "1. 64 x 128 영상에서 계산\n",
    "\n",
    "\n",
    "2. 입력 영상으로부터 그래디언트 계산 (크기, 방향 성분 0~180도)\n",
    "\n",
    "\n",
    "\n",
    "3. 입력 영상을 8x8 크기 단위로 분할 (가로 8, 세로 16개)\n",
    "\n",
    "\n",
    "4. 각각의 셀로부터 그래디언트 방향 성분에 대한 히스토그램 계산\n",
    "\n",
    "   - 방향 성분을 20도 단위로 구분하여 총 9개의 bin으로 구성된 방향 히스토그램 구성\n",
    "   \n",
    "   \n",
    "   \n",
    "5. 인접한 4개의 셀을 합쳐서 블록이라고 정의\n",
    "\n",
    "   - 하나의 블록에는 4개의 셀, 각 셀에는 9개의 bin으로 구성된 방향 히스토그램 정보\n",
    "   \n",
    "   - 블록 하나에는 총 36개의 실수 값으로 구성된 방향 히스토그램 정보\n",
    "   \n",
    "   \n",
    "   \n",
    "6. 블록은 가로, 세로 방향으로 각각 한 개의 셀 만큼 이동하면서 정의\n",
    "\n",
    "   - 가로 방향으로 7개, 세로 방향으로 15개 정의 가능\n",
    "   \n",
    "   - 64 x 128 입력 영상에서 총 105개의 블록 추출, 전체 블록에서 추출되는 방향 히스토그램 실수 값은 105 x 36 = 3780개\n",
    "   \n",
    "   \n",
    "7. 3780개의 실수 값이 64 x 128 영상을 표현하는 HOG 특징 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b81f4",
   "metadata": {},
   "source": [
    "**HOG 기술자 객체 생성 및 학습된 분류기 계수 불러오기 - cv2.HOGDescriptor**\n",
    "\n",
    "\n",
    "cv2.HOGDescriptor()  ->  HOGDescriptor object\n",
    "\n",
    "cv2.HOGDescriptor_getDefaultPeopleDetector() -> retval (미리 훈련된 특징 벡터 가져오기)\n",
    "\n",
    "`retval` : 미리 훈련된 특징 벡터. numpy.ndarray. shape=(3781, 1). dtype=numpy.float32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f13398",
   "metadata": {},
   "source": [
    "**HOG 멀티스케일 객체 검출 함수**\n",
    "\n",
    "cv2.HOGDescriptor.detectMultiScale(img, hitThreshold=None, winStride=None, padding=None, scale=None, finalThreshold=None, useMeanshiftGrouping=None) -> foundLocations, foundWeights\n",
    "\n",
    "`img` : 입력 영상. cv2.CV_8UC1 또는 cv2.CV_8UC3.\n",
    "\n",
    "`hitThreshold` : 특징 벡터와 SVM 분류 평면까지의 거리에 대한 임계값\n",
    "\n",
    "`winStride` : 셀 윈도우 이동 크기. (0, 0) 지정 시 셀 크기와 같게 설정.\n",
    "\n",
    "`padding` : 패딩 크기\n",
    "\n",
    "`scale` : 검색 윈도우 크기 확대 비율. 기본값은 1.05.\n",
    "\n",
    "`finalThreshold` : 검출 결정을 위한 임계값\n",
    "\n",
    "`useMeanshiftGrouping` : 겹쳐진 검색 윈도우를 합치는 방법 지정 플래그\n",
    "\n",
    "`foundLocations` : (출력) 검출된 사각형 영역 정보\n",
    "\n",
    "`foundWeights` : (출력) 검출된 사각형 영역에 대한 신뢰도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cda2e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('C:/Users/ky_moon/Desktop/vision/ch08/vtest.avi')\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Video open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 보행자 검출을 위한 HOG 기술자 설정\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 매 프레임마다 보행자 검출\n",
    "    detected, _ = hog.detectMultiScale(frame)\n",
    "\n",
    "    # 검출 결과 화면 표시\n",
    "    for (x, y, w, h) in detected:\n",
    "        c = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        cv2.rectangle(frame, (x, y, w, h), c, 3)\n",
    "\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(10) == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7352f",
   "metadata": {},
   "source": [
    "## SnowAPP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e12db",
   "metadata": {},
   "source": [
    "캐스케이드 분류기 사용\n",
    "\n",
    "  - XML : https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "  \n",
    "  - 얼굴 검출 XML : haarcascade_frontalface_alt2.xml\n",
    "  \n",
    "  - 눈 검출 XML : haarcascade_eye.xml\n",
    "  \n",
    "  \n",
    "눈 검출\n",
    "\n",
    "  - 얼굴 검출 영역 내에서만 눈 검출\n",
    "  \n",
    "  - 2개 검출되었을 때만 그래픽 합성\n",
    "  \n",
    "  - 양안의 눈 좌표를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4033b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "# 3채널 img 영상에 4채널 item 영상을 pos 위치에 합성\n",
    "def overlay(img, glasses, pos):\n",
    "    # 실제 합성을 수행할 부분 영상 좌표 계산\n",
    "    sx = pos[0]  # start x\n",
    "    ex = pos[0] + glasses.shape[1]  # start x + glasses.width\n",
    "    sy = pos[1]  # start y\n",
    "    ey = pos[1] + glasses.shape[0]  # start y + glasses.height\n",
    "\n",
    "    # 합성할 영역이 입력 영상 크기를 벗어나면 무시\n",
    "    if sx < 0 or sy < 0 or ex > img.shape[1] or ey > img.shape[0]:\n",
    "        return\n",
    "\n",
    "    # 부분 영상 참조. img1: 입력 영상의 부분 영상, img2: 안경 영상의 부분 영상\n",
    "    img1 = img[sy:ey, sx:ex]   # shape=(h, w, 3)\n",
    "    img2 = glasses[:, :, 0:3]  # shape=(h, w, 3)\n",
    "    alpha = 1. - (glasses[:, :, 3] / 255.)  # shape=(h, w)\n",
    "\n",
    "    # BGR 채널별로 두 부분 영상의 가중합\n",
    "    img1[..., 0] = (img1[..., 0] * alpha + img2[..., 0] * (1. - alpha)).astype(np.uint8)\n",
    "    img1[..., 1] = (img1[..., 1] * alpha + img2[..., 1] * (1. - alpha)).astype(np.uint8)\n",
    "    img1[..., 2] = (img1[..., 2] * alpha + img2[..., 2] * (1. - alpha)).astype(np.uint8)\n",
    "\n",
    "\n",
    "# 카메라 열기\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print('Camera open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "out = cv2.VideoWriter('C:/Users/ky_moon/Desktop/vision/ch08/output.avi', fourcc, 30, (w, h))\n",
    "\n",
    "# Haar-like XML 파일 열기\n",
    "face_classifier = cv2.CascadeClassifier('C:/Users/ky_moon/Desktop/vision/ch08/haarcascade_frontalface_alt2.xml')\n",
    "eye_classifier = cv2.CascadeClassifier('C:/Users/ky_moon/Desktop/vision/ch08/haarcascade_eye.xml')\n",
    "\n",
    "if face_classifier.empty() or eye_classifier.empty():\n",
    "    print('XML load failed!')\n",
    "    sys.exit()\n",
    "\n",
    "# 안경 PNG 파일 열기 (Image from http://www.pngall.com/)\n",
    "glasses = cv2.imread('C:/Users/ky_moon/Desktop/vision/ch08/glasses.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "if glasses is None:\n",
    "    print('PNG image open failed!')\n",
    "    sys.exit()\n",
    "\n",
    "ew, eh = glasses.shape[:2]  # 가로, 세로 크기\n",
    "ex1, ey1 = 240, 300  # 왼쪽 눈 좌표\n",
    "ex2, ey2 = 660, 300  # 오른쪽 눈 좌표\n",
    "\n",
    "# 매 프레임에 대해 얼굴 검출 및 안경 합성\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 얼굴 검출\n",
    "    faces = face_classifier.detectMultiScale(frame, scaleFactor=1.2,\n",
    "                                             minSize=(100, 100), maxSize=(400, 400))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        #cv2.rectangle(frame, (x, y, w, h), (255, 0, 255), 2)\n",
    "\n",
    "        # 얼굴 영역에서 눈 검출\n",
    "        faceROI = frame[y:y + h // 2, x:x + w]\n",
    "        eyes = eye_classifier.detectMultiScale(faceROI)\n",
    "\n",
    "        # 눈을 2개 검출한 것이 아니라면 무시\n",
    "        if len(eyes) != 2:\n",
    "            continue\n",
    "\n",
    "        # 두 개의 눈 중앙 위치를 (x1, y1), (x2, y2) 좌표로 저장\n",
    "        # 눈의 좌표는 얼굴 영상에서 계산된 것이기 때문에 전체 영상의 눈 좌표를 계산 (가로, 세로 크기의 반을 추가적으로 더해줌)\n",
    "        x1 = x + eyes[0][0] + (eyes[0][2] // 2)\n",
    "        y1 = y + eyes[0][1] + (eyes[0][3] // 2)\n",
    "        x2 = x + eyes[1][0] + (eyes[1][2] // 2)\n",
    "        y2 = y + eyes[1][1] + (eyes[1][3] // 2)\n",
    "        \n",
    "        # 왼 눈, 오른 눈 순서 맞추기\n",
    "        if x1 > x2:\n",
    "            x1, y1, x2, y2 = x2, y2, x1, y1\n",
    "\n",
    "        #cv2.circle(faceROI, (x1, y1), 5, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        #cv2.circle(faceROI, (x2, y2), 5, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # 두 눈 사이의 거리를 이용하여 스케일링 팩터를 계산 (두 눈이 수평하다고 가정)\n",
    "        # 실제 입력 영상에서 두 눈의 좌표 간격, 안경 영상에서 두 눈의 좌표 간격을 이용하여 resize factor 지정\n",
    "        fx = (x2 - x1) / (ex2 - ex1)\n",
    "        glasses2 = cv2.resize(glasses, (0, 0), fx=fx, fy=fx, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # 크기 조절된 안경 영상을 합성할 위치 계산 (좌상단 좌표)\n",
    "        pos = (x1 - int(ex1 * fx), y1 - int(ey1 * fx))\n",
    "\n",
    "        # 영상 합성\n",
    "        overlay(frame, glasses2, pos)\n",
    "\n",
    "    # 프레임 저장 및 화면 출력\n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae3288c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e06abe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[181, 182, 182, ..., 139, 139, 139],\n",
       "       [180, 181, 182, ..., 138, 138, 138],\n",
       "       [181, 182, 182, ..., 136, 136, 137],\n",
       "       ...,\n",
       "       [ 87,  87,  87, ..., 100, 100,  99],\n",
       "       [ 87,  88,  88, ..., 100,  99,  99],\n",
       "       [ 87,  88,  89, ..., 100,  99,  98]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44d358ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[...,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65dabe1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182, 183, 183, ..., 135, 135, 135],\n",
       "       [182, 183, 184, ..., 136, 136, 136],\n",
       "       [183, 184, 184, ..., 137, 137, 137],\n",
       "       ...,\n",
       "       [ 85,  85,  84, ...,  98,  98,  97],\n",
       "       [ 85,  86,  85, ...,  98,  97,  97],\n",
       "       [ 85,  86,  86, ...,  98,  97,  96]], dtype=uint8)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfeb420d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[173, 174, 174, ..., 140, 140, 140],\n",
       "       [173, 174, 175, ..., 139, 139, 139],\n",
       "       [174, 175, 175, ..., 138, 138, 140],\n",
       "       ...,\n",
       "       [ 75,  75,  76, ...,  82,  82,  81],\n",
       "       [ 75,  76,  77, ...,  82,  81,  81],\n",
       "       [ 75,  76,  78, ...,  82,  81,  80]], dtype=uint8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65116d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 2 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-5b68bdb969b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 2 with size 3"
     ]
    }
   ],
   "source": [
    "frame[..., 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a937162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
